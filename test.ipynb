{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ ftfy already installed\n",
      "✓ pycocotools already installed\n",
      "✓ fvcore already installed\n",
      "✓ h5py already installed\n",
      "✓ pybase64 already installed\n",
      "✓ tqdm already installed\n",
      "Installing torchtext...\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = ['ftfy', 'pycocotools', 'fvcore', 'h5py', 'pybase64', 'tqdm', 'torchtext']\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "        print(f\"✓ {package} installed\")\n",
    "\n",
    "print(\"\\nAll dependencies installed successfully!\")\n",
    "\n",
    "# Add the ml-cvnets repository root to Python path\n",
    "import os\n",
    "notebook_dir = r'd:\\Git_repos\\other-repos\\ml-cvnets'\n",
    "\n",
    "if notebook_dir not in sys.path:\n",
    "    sys.path.insert(0, notebook_dir)\n",
    "\n",
    "print(f\"\\nAdded {notebook_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8781d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MobileViT for CIFAR-10 training...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import from ml-cvnets repository\n",
    "from cvnets import get_model\n",
    "from options.opts import get_training_arguments\n",
    "\n",
    "# Create argument namespace for model configuration\n",
    "def create_opts_for_mobilevit(num_classes=10, mode='xx_small'):\n",
    "    \"\"\"Create options object for MobileViT model\"\"\"\n",
    "    parser = get_training_arguments(parse_args=False)\n",
    "    opts = parser.parse_args([])\n",
    "    \n",
    "    # Basic settings\n",
    "    setattr(opts, 'common.config_file', None)\n",
    "    setattr(opts, 'common.results_loc', './results')\n",
    "    setattr(opts, 'common.override_kwargs', None)\n",
    "    \n",
    "    # Dataset settings\n",
    "    setattr(opts, 'dataset.category', 'classification')\n",
    "    setattr(opts, 'dataset.name', 'cifar10')\n",
    "    \n",
    "    # Model settings\n",
    "    setattr(opts, 'model.classification.name', 'mobilevit')\n",
    "    setattr(opts, 'model.classification.n_classes', num_classes)\n",
    "    setattr(opts, 'model.classification.classifier_dropout', 0.1)\n",
    "    setattr(opts, 'model.classification.mit.mode', mode)  # xx_small, x_small, or small\n",
    "    setattr(opts, 'model.classification.mit.head_dim', None)\n",
    "    setattr(opts, 'model.classification.mit.number_heads', 4)\n",
    "    setattr(opts, 'model.classification.mit.attn_dropout', 0.0)\n",
    "    setattr(opts, 'model.classification.mit.ffn_dropout', 0.0)\n",
    "    setattr(opts, 'model.classification.mit.dropout', 0.1)\n",
    "    setattr(opts, 'model.classification.mit.no_fuse_local_global_features', False)\n",
    "    setattr(opts, 'model.classification.mit.conv_kernel_size', 3)\n",
    "    \n",
    "    # Activation and normalization\n",
    "    setattr(opts, 'model.classification.activation.name', 'swish')\n",
    "    setattr(opts, 'model.normalization.name', 'batch_norm')\n",
    "    setattr(opts, 'model.normalization.momentum', 0.1)\n",
    "    setattr(opts, 'model.activation.name', 'swish')\n",
    "    \n",
    "    # Layer settings\n",
    "    setattr(opts, 'model.layer.global_pool', 'mean')\n",
    "    setattr(opts, 'model.layer.conv_init', 'kaiming_normal')\n",
    "    setattr(opts, 'model.layer.linear_init', 'trunc_normal')\n",
    "    setattr(opts, 'model.layer.linear_init_std_dev', 0.02)\n",
    "    \n",
    "    return opts\n",
    "\n",
    "print(\"Setting up MobileViT for CIFAR-10 training...\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cfa2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Memory: 25.76 GB\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "# Using smaller model (xx_small) for faster training on home setup\n",
    "# You can try 'x_small' or 'small' for better accuracy (but slower training)\n",
    "MODEL_MODE = 'xx_small'  # Options: 'xx_small', 'x_small', 'small'\n",
    "NUM_EPOCHS = 100  # Paper uses 300 for ImageNet, but 100-200 is common for CIFAR-10\n",
    "BATCH_SIZE = 128  # Adjust based on your GPU memory\n",
    "LEARNING_RATE = 0.002  # Will be adjusted by warmup and cosine schedule\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_CLASSES = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03cd27ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data loaders...\n",
      "Attempting to load CIFAR-10 dataset (attempt 1/3)...\n",
      "Download failed: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>\n",
      "Retrying in 2 seconds...\n",
      "Download failed: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>\n",
      "Retrying in 2 seconds...\n",
      "Attempting to load CIFAR-10 dataset (attempt 2/3)...\n",
      "Attempting to load CIFAR-10 dataset (attempt 2/3)...\n",
      "Download failed: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>\n",
      "Retrying in 2 seconds...\n",
      "Download failed: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>\n",
      "Retrying in 2 seconds...\n",
      "Attempting to load CIFAR-10 dataset (attempt 3/3)...\n",
      "Attempting to load CIFAR-10 dataset (attempt 3/3)...\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [15:46<00:00, 180101.85it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "✓ Dataset loaded successfully!\n",
      "\n",
      "Dataset Statistics:\n",
      "  Training samples: 50000\n",
      "  Test samples: 10000\n",
      "  Training batches: 391\n",
      "  Test batches: 79\n",
      "✓ Dataset loaded successfully!\n",
      "\n",
      "Dataset Statistics:\n",
      "  Training samples: 50000\n",
      "  Test samples: 10000\n",
      "  Training batches: 391\n",
      "  Test batches: 79\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for CIFAR-10\n",
    "# Following standard practices and paper's recommendations\n",
    "print(\"Preparing data loaders...\")\n",
    "\n",
    "# CIFAR-10 statistics\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Test transforms (no augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with retry logic\n",
    "import time\n",
    "max_retries = 3\n",
    "retry_delay = 2\n",
    "\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        print(f\"Attempting to load CIFAR-10 dataset (attempt {attempt + 1}/{max_retries})...\")\n",
    "        \n",
    "        train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', \n",
    "            train=True, \n",
    "            download=True, \n",
    "            transform=train_transform\n",
    "        )\n",
    "        \n",
    "        test_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', \n",
    "            train=False, \n",
    "            download=True, \n",
    "            transform=test_transform\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Dataset loaded successfully!\")\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            print(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(f\"\\n✗ Failed to download CIFAR-10 after {max_retries} attempts.\")\n",
    "            print(\"Please manually download CIFAR-10 from:\")\n",
    "            print(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\")\n",
    "            print(\"Extract it to ./data/cifar-10-batches-py/\")\n",
    "            raise\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e966f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing MobileViT model...\n",
      "2025-12-07 15:57:52 - \u001b[93m\u001b[1mDEBUG   \u001b[0m - Cannot load internal arguments, skipping.\n",
      "2025-12-07 15:57:52 - \u001b[93m\u001b[1mDEBUG   \u001b[0m - Cannot load internal arguments, skipping.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create MobileViT model using the repository's infrastructure\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInitializing MobileViT model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m opts \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_opts_for_mobilevit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_MODE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m get_model(opts)\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mcreate_opts_for_mobilevit\u001b[1;34m(num_classes, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_opts_for_mobilevit\u001b[39m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxx_small\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create options object for MobileViT model\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     parser \u001b[38;5;241m=\u001b[39m \u001b[43mget_training_arguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     opts \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args([])\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Basic settings\u001b[39;00m\n",
      "File \u001b[1;32md:\\Git_repos\\other-repos\\ml-cvnets\\options\\opts.py:329\u001b[0m, in \u001b[0;36mget_training_arguments\u001b[1;34m(parse_args, args)\u001b[0m\n\u001b[0;32m    326\u001b[0m parser \u001b[38;5;241m=\u001b[39m arguments_common(parser\u001b[38;5;241m=\u001b[39mparser)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# text tokenizer arguments\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43marguments_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# metric arguments\u001b[39;00m\n\u001b[0;32m    332\u001b[0m parser \u001b[38;5;241m=\u001b[39m METRICS_REGISTRY\u001b[38;5;241m.\u001b[39mall_arguments(parser\u001b[38;5;241m=\u001b[39mparser)\n",
      "File \u001b[1;32md:\\Git_repos\\other-repos\\ml-cvnets\\data\\text_tokenizer\\__init__.py:25\u001b[0m, in \u001b[0;36marguments_tokenizer\u001b[1;34m(parser)\u001b[0m\n\u001b[0;32m     22\u001b[0m parser \u001b[38;5;241m=\u001b[39m BaseTokenizer\u001b[38;5;241m.\u001b[39madd_arguments(parser)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# add class specific arguments\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTOKENIZER_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_arguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\Git_repos\\other-repos\\ml-cvnets\\utils\\registry.py:180\u001b[0m, in \u001b[0;36mRegistry.all_arguments\u001b[1;34m(self, parser)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_arguments\u001b[39m(\u001b[38;5;28mself\u001b[39m, parser: argparse\u001b[38;5;241m.\u001b[39mArgumentParser) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser:\n\u001b[0;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    Iterates through all items and fetches their arguments.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    Note: make sure that all items are already registered before calling this method.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marguments_accessed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32md:\\Git_repos\\other-repos\\ml-cvnets\\utils\\registry.py:97\u001b[0m, in \u001b[0;36mRegistry._load_all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dir_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_dirs:\n\u001b[1;32m---> 97\u001b[0m     \u001b[43mimport_modules_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_roots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minternal_dirs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Git_repos\\other-repos\\ml-cvnets\\utils\\import_utils.py:41\u001b[0m, in \u001b[0;36mimport_modules_from_folder\u001b[1;34m(folder_name, extra_roots)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     38\u001b[0m     module_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m     39\u001b[0m         path\u001b[38;5;241m.\u001b[39mrelative_to(LIBRARY_ROOT)\u001b[38;5;241m.\u001b[39mwith_suffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m     )\u001b[38;5;241m.\u001b[39mreplace(os\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32md:\\Git_repos\\other-repos\\ml-cvnets\\data\\text_tokenizer\\clip_tokenizer.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPTokenizer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_REGISTRY, BaseTokenizer\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "# Create MobileViT model using the repository's infrastructure\n",
    "print(\"\\nInitializing MobileViT model...\")\n",
    "opts = create_opts_for_mobilevit(num_classes=NUM_CLASSES, mode=MODEL_MODE)\n",
    "\n",
    "try:\n",
    "    model = get_model(opts)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model information\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model: MobileViT-{MODEL_MODE}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: ~{total_params * 4 / 1e6:.2f} MB (FP32)\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    dummy_input = torch.randn(2, 3, 32, 32).to(device)\n",
    "    with torch.no_grad():\n",
    "        dummy_output = model(dummy_input)\n",
    "    print(f\"Output shape: {dummy_output.shape}\")\n",
    "    print(\"✓ Model created successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe77f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and learning rate scheduler\n",
    "# Following the paper's training recipe (AdamW with cosine schedule)\n",
    "print(\"\\nSetting up training components...\")\n",
    "\n",
    "# Loss function with label smoothing (as in the paper)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# AdamW optimizer (as in the paper)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Cosine annealing learning rate scheduler with warmup\n",
    "# Paper uses 20k warmup iterations, we'll use 5 epochs for CIFAR-10\n",
    "warmup_epochs = 5\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = len(train_loader) * warmup_epochs\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup\n",
    "        return (step / warmup_steps)\n",
    "    else:\n",
    "        # Cosine annealing\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr)\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
    "print(f\"Scheduler: Cosine annealing with {warmup_epochs} epoch warmup\")\n",
    "print(f\"Loss: CrossEntropyLoss with label smoothing 0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if batch_idx % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%',\n",
    "                'lr': f'{current_lr:.6f}'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc='Evaluating', leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    return test_loss, test_acc\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with checkpointing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Evaluate every 5 epochs or at the last epoch\n",
    "        if (epoch + 1) % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "            test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            # Save history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}:')\n",
    "            print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "            print(f'  Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "            print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_acc': best_acc,\n",
    "                    'history': history,\n",
    "                }, './checkpoints/mobilevit_cifar10_best.pth')\n",
    "                print(f'  ✓ Best model saved! (Acc: {best_acc:.2f}%)')\n",
    "        \n",
    "        # Save checkpoint every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history': history,\n",
    "            }, f'./checkpoints/mobilevit_cifar10_epoch_{epoch+1}.pth')\n",
    "            print(f'  Checkpoint saved at epoch {epoch+1}')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training Completed!\")\n",
    "    print(f\"Best Test Accuracy: {best_acc:.2f}% at epoch {best_epoch}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining interrupted by user.\")\n",
    "    print(f\"Best accuracy so far: {best_acc:.2f}% at epoch {best_epoch}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\nError during training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68756300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if len(history['train_loss']) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['test_loss'], label='Test Loss', marker='s')\n",
    "    axes[0].set_xlabel('Evaluation Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Test Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "    axes[1].plot(history['test_acc'], label='Test Acc', marker='s')\n",
    "    axes[1].set_xlabel('Evaluation Step')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Test Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Plot learning rate\n",
    "    axes[2].plot(history['lr'], marker='o', color='orange')\n",
    "    axes[2].set_xlabel('Evaluation Step')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('Learning Rate Schedule')\n",
    "    axes[2].grid(True)\n",
    "    axes[2].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./checkpoints/training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"  Best Test Accuracy: {best_acc:.2f}%\")\n",
    "    print(f\"  Final Train Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "    print(f\"  Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"\\nModel: MobileViT-{MODEL_MODE}\")\n",
    "    print(f\"Parameters: {total_params:,}\")\n",
    "    print(f\"Training completed in {NUM_EPOCHS} epochs\")\n",
    "else:\n",
    "    print(\"No training history to plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
