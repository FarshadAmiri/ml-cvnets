{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad8580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ ftfy already installed\n",
      "✓ pycocotools already installed\n",
      "✓ fvcore already installed\n",
      "✓ h5py already installed\n",
      "✓ pybase64 already installed\n",
      "✓ tqdm already installed\n",
      "Installing torchtext...\n",
      "✓ torchtext installed\n",
      "\n",
      "All dependencies installed successfully!\n",
      "\n",
      "Added d:\\Git_repos\\other-repos\\ml-cvnets to Python path\n",
      "✓ torchtext installed\n",
      "\n",
      "All dependencies installed successfully!\n",
      "\n",
      "Added d:\\Git_repos\\other-repos\\ml-cvnets to Python path\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = ['ftfy', 'pycocotools', 'fvcore', 'h5py', 'pybase64', 'tqdm', 'torchtext']\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"✓ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "        print(f\"✓ {package} installed\")\n",
    "\n",
    "print(\"\\nAll dependencies installed successfully!\")\n",
    "\n",
    "# Add the ml-cvnets repository root to Python path\n",
    "import os\n",
    "notebook_dir = r'd:\\Git_repos\\other-repos\\ml-cvnets'\n",
    "\n",
    "if notebook_dir not in sys.path:\n",
    "    sys.path.insert(0, notebook_dir)\n",
    "\n",
    "print(f\"\\nAdded {notebook_dir} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8781d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MobileViT for CIFAR-10 training...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import from ml-cvnets repository\n",
    "from cvnets import get_model\n",
    "from options.opts import get_training_arguments\n",
    "\n",
    "# Create argument namespace for model configuration\n",
    "def create_opts_for_mobilevit(num_classes=10, mode='xx_small'):\n",
    "    \"\"\"Create options object for MobileViT model\"\"\"\n",
    "    parser = get_training_arguments(parse_args=False)\n",
    "    opts = parser.parse_args([])\n",
    "    \n",
    "    # Basic settings\n",
    "    setattr(opts, 'common.config_file', None)\n",
    "    setattr(opts, 'common.results_loc', './results')\n",
    "    setattr(opts, 'common.override_kwargs', None)\n",
    "    \n",
    "    # Dataset settings\n",
    "    setattr(opts, 'dataset.category', 'classification')\n",
    "    setattr(opts, 'dataset.name', 'cifar10')\n",
    "    \n",
    "    # Model settings\n",
    "    setattr(opts, 'model.classification.name', 'mobilevit')\n",
    "    setattr(opts, 'model.classification.n_classes', num_classes)\n",
    "    setattr(opts, 'model.classification.classifier_dropout', 0.1)\n",
    "    setattr(opts, 'model.classification.mit.mode', mode)  # xx_small, x_small, or small\n",
    "    setattr(opts, 'model.classification.mit.head_dim', None)\n",
    "    setattr(opts, 'model.classification.mit.number_heads', 4)\n",
    "    setattr(opts, 'model.classification.mit.attn_dropout', 0.0)\n",
    "    setattr(opts, 'model.classification.mit.ffn_dropout', 0.0)\n",
    "    setattr(opts, 'model.classification.mit.dropout', 0.1)\n",
    "    setattr(opts, 'model.classification.mit.no_fuse_local_global_features', False)\n",
    "    setattr(opts, 'model.classification.mit.conv_kernel_size', 3)\n",
    "    \n",
    "    # Activation and normalization\n",
    "    setattr(opts, 'model.classification.activation.name', 'swish')\n",
    "    setattr(opts, 'model.normalization.name', 'batch_norm')\n",
    "    setattr(opts, 'model.normalization.momentum', 0.1)\n",
    "    setattr(opts, 'model.activation.name', 'swish')\n",
    "    \n",
    "    # Layer settings\n",
    "    setattr(opts, 'model.layer.global_pool', 'mean')\n",
    "    setattr(opts, 'model.layer.conv_init', 'kaiming_normal')\n",
    "    setattr(opts, 'model.layer.linear_init', 'trunc_normal')\n",
    "    setattr(opts, 'model.layer.linear_init_std_dev', 0.02)\n",
    "    \n",
    "    return opts\n",
    "\n",
    "print(\"Setting up MobileViT for CIFAR-10 training...\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8cfa2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Memory: 25.76 GB\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "# Using smaller model (xx_small) for faster training on home setup\n",
    "# You can try 'x_small' or 'small' for better accuracy (but slower training)\n",
    "MODEL_MODE = 'xx_small'  # Options: 'xx_small', 'x_small', 'small'\n",
    "NUM_EPOCHS = 100  # Paper uses 300 for ImageNet, but 100-200 is common for CIFAR-10\n",
    "BATCH_SIZE = 128  # Adjust based on your GPU memory\n",
    "LEARNING_RATE = 0.002  # Will be adjusted by warmup and cosine schedule\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_CLASSES = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03cd27ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data loaders...\n",
      "Attempting to load CIFAR-10 dataset (attempt 1/3)...\n",
      "Download failed: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>\n",
      "Retrying in 2 seconds...\n",
      "Download failed: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>\n",
      "Retrying in 2 seconds...\n",
      "Attempting to load CIFAR-10 dataset (attempt 2/3)...\n",
      "Attempting to load CIFAR-10 dataset (attempt 2/3)...\n",
      "Download failed: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>\n",
      "Retrying in 2 seconds...\n",
      "Download failed: <urlopen error [WinError 10061] No connection could be made because the target machine actively refused it>\n",
      "Retrying in 2 seconds...\n",
      "Attempting to load CIFAR-10 dataset (attempt 3/3)...\n",
      "Attempting to load CIFAR-10 dataset (attempt 3/3)...\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [15:46<00:00, 180101.85it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "✓ Dataset loaded successfully!\n",
      "\n",
      "Dataset Statistics:\n",
      "  Training samples: 50000\n",
      "  Test samples: 10000\n",
      "  Training batches: 391\n",
      "  Test batches: 79\n",
      "✓ Dataset loaded successfully!\n",
      "\n",
      "Dataset Statistics:\n",
      "  Training samples: 50000\n",
      "  Test samples: 10000\n",
      "  Training batches: 391\n",
      "  Test batches: 79\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for CIFAR-10\n",
    "# Following standard practices and paper's recommendations\n",
    "print(\"Preparing data loaders...\")\n",
    "\n",
    "# CIFAR-10 statistics\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Test transforms (no augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset with retry logic\n",
    "import time\n",
    "max_retries = 3\n",
    "retry_delay = 2\n",
    "\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        print(f\"Attempting to load CIFAR-10 dataset (attempt {attempt + 1}/{max_retries})...\")\n",
    "        \n",
    "        train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', \n",
    "            train=True, \n",
    "            download=True, \n",
    "            transform=train_transform\n",
    "        )\n",
    "        \n",
    "        test_dataset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', \n",
    "            train=False, \n",
    "            download=True, \n",
    "            transform=test_transform\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Dataset loaded successfully!\")\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            print(f\"Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(f\"\\n✗ Failed to download CIFAR-10 after {max_retries} attempts.\")\n",
    "            print(\"Please manually download CIFAR-10 from:\")\n",
    "            print(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\")\n",
    "            print(\"Extract it to ./data/cifar-10-batches-py/\")\n",
    "            raise\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e966f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing MobileViT model...\n",
      "2025-12-07 15:58:24 - \u001b[93m\u001b[1mDEBUG   \u001b[0m - Cannot load internal arguments, skipping.\n",
      "2025-12-07 15:58:35 - \u001b[32m\u001b[1mINFO   \u001b[0m - Trainable parameters: ['conv_1.block.conv.weight', 'conv_1.block.norm.weight', 'conv_1.block.norm.bias', 'layer_1.0.block.exp_1x1.block.conv.weight', 'layer_1.0.block.exp_1x1.block.norm.weight', 'layer_1.0.block.exp_1x1.block.norm.bias', 'layer_1.0.block.conv_3x3.block.conv.weight', 'layer_1.0.block.conv_3x3.block.norm.weight', 'layer_1.0.block.conv_3x3.block.norm.bias', 'layer_1.0.block.red_1x1.block.conv.weight', 'layer_1.0.block.red_1x1.block.norm.weight', 'layer_1.0.block.red_1x1.block.norm.bias', 'layer_2.0.block.exp_1x1.block.conv.weight', 'layer_2.0.block.exp_1x1.block.norm.weight', 'layer_2.0.block.exp_1x1.block.norm.bias', 'layer_2.0.block.conv_3x3.block.conv.weight', 'layer_2.0.block.conv_3x3.block.norm.weight', 'layer_2.0.block.conv_3x3.block.norm.bias', 'layer_2.0.block.red_1x1.block.conv.weight', 'layer_2.0.block.red_1x1.block.norm.weight', 'layer_2.0.block.red_1x1.block.norm.bias', 'layer_2.1.block.exp_1x1.block.conv.weight', 'layer_2.1.block.exp_1x1.block.norm.weight', 'layer_2.1.block.exp_1x1.block.norm.bias', 'layer_2.1.block.conv_3x3.block.conv.weight', 'layer_2.1.block.conv_3x3.block.norm.weight', 'layer_2.1.block.conv_3x3.block.norm.bias', 'layer_2.1.block.red_1x1.block.conv.weight', 'layer_2.1.block.red_1x1.block.norm.weight', 'layer_2.1.block.red_1x1.block.norm.bias', 'layer_2.2.block.exp_1x1.block.conv.weight', 'layer_2.2.block.exp_1x1.block.norm.weight', 'layer_2.2.block.exp_1x1.block.norm.bias', 'layer_2.2.block.conv_3x3.block.conv.weight', 'layer_2.2.block.conv_3x3.block.norm.weight', 'layer_2.2.block.conv_3x3.block.norm.bias', 'layer_2.2.block.red_1x1.block.conv.weight', 'layer_2.2.block.red_1x1.block.norm.weight', 'layer_2.2.block.red_1x1.block.norm.bias', 'layer_3.0.block.exp_1x1.block.conv.weight', 'layer_3.0.block.exp_1x1.block.norm.weight', 'layer_3.0.block.exp_1x1.block.norm.bias', 'layer_3.0.block.conv_3x3.block.conv.weight', 'layer_3.0.block.conv_3x3.block.norm.weight', 'layer_3.0.block.conv_3x3.block.norm.bias', 'layer_3.0.block.red_1x1.block.conv.weight', 'layer_3.0.block.red_1x1.block.norm.weight', 'layer_3.0.block.red_1x1.block.norm.bias', 'layer_3.1.local_rep.conv_3x3.block.conv.weight', 'layer_3.1.local_rep.conv_3x3.block.norm.weight', 'layer_3.1.local_rep.conv_3x3.block.norm.bias', 'layer_3.1.local_rep.conv_1x1.block.conv.weight', 'layer_3.1.global_rep.0.pre_norm_mha.0.weight', 'layer_3.1.global_rep.0.pre_norm_mha.0.bias', 'layer_3.1.global_rep.0.pre_norm_mha.1.qkv_proj.weight', 'layer_3.1.global_rep.0.pre_norm_mha.1.qkv_proj.bias', 'layer_3.1.global_rep.0.pre_norm_mha.1.out_proj.weight', 'layer_3.1.global_rep.0.pre_norm_mha.1.out_proj.bias', 'layer_3.1.global_rep.0.pre_norm_ffn.0.weight', 'layer_3.1.global_rep.0.pre_norm_ffn.0.bias', 'layer_3.1.global_rep.0.pre_norm_ffn.1.weight', 'layer_3.1.global_rep.0.pre_norm_ffn.1.bias', 'layer_3.1.global_rep.0.pre_norm_ffn.4.weight', 'layer_3.1.global_rep.0.pre_norm_ffn.4.bias', 'layer_3.1.global_rep.1.pre_norm_mha.0.weight', 'layer_3.1.global_rep.1.pre_norm_mha.0.bias', 'layer_3.1.global_rep.1.pre_norm_mha.1.qkv_proj.weight', 'layer_3.1.global_rep.1.pre_norm_mha.1.qkv_proj.bias', 'layer_3.1.global_rep.1.pre_norm_mha.1.out_proj.weight', 'layer_3.1.global_rep.1.pre_norm_mha.1.out_proj.bias', 'layer_3.1.global_rep.1.pre_norm_ffn.0.weight', 'layer_3.1.global_rep.1.pre_norm_ffn.0.bias', 'layer_3.1.global_rep.1.pre_norm_ffn.1.weight', 'layer_3.1.global_rep.1.pre_norm_ffn.1.bias', 'layer_3.1.global_rep.1.pre_norm_ffn.4.weight', 'layer_3.1.global_rep.1.pre_norm_ffn.4.bias', 'layer_3.1.global_rep.2.weight', 'layer_3.1.global_rep.2.bias', 'layer_3.1.conv_proj.block.conv.weight', 'layer_3.1.conv_proj.block.norm.weight', 'layer_3.1.conv_proj.block.norm.bias', 'layer_3.1.fusion.block.conv.weight', 'layer_3.1.fusion.block.norm.weight', 'layer_3.1.fusion.block.norm.bias', 'layer_4.0.block.exp_1x1.block.conv.weight', 'layer_4.0.block.exp_1x1.block.norm.weight', 'layer_4.0.block.exp_1x1.block.norm.bias', 'layer_4.0.block.conv_3x3.block.conv.weight', 'layer_4.0.block.conv_3x3.block.norm.weight', 'layer_4.0.block.conv_3x3.block.norm.bias', 'layer_4.0.block.red_1x1.block.conv.weight', 'layer_4.0.block.red_1x1.block.norm.weight', 'layer_4.0.block.red_1x1.block.norm.bias', 'layer_4.1.local_rep.conv_3x3.block.conv.weight', 'layer_4.1.local_rep.conv_3x3.block.norm.weight', 'layer_4.1.local_rep.conv_3x3.block.norm.bias', 'layer_4.1.local_rep.conv_1x1.block.conv.weight', 'layer_4.1.global_rep.0.pre_norm_mha.0.weight', 'layer_4.1.global_rep.0.pre_norm_mha.0.bias', 'layer_4.1.global_rep.0.pre_norm_mha.1.qkv_proj.weight', 'layer_4.1.global_rep.0.pre_norm_mha.1.qkv_proj.bias', 'layer_4.1.global_rep.0.pre_norm_mha.1.out_proj.weight', 'layer_4.1.global_rep.0.pre_norm_mha.1.out_proj.bias', 'layer_4.1.global_rep.0.pre_norm_ffn.0.weight', 'layer_4.1.global_rep.0.pre_norm_ffn.0.bias', 'layer_4.1.global_rep.0.pre_norm_ffn.1.weight', 'layer_4.1.global_rep.0.pre_norm_ffn.1.bias', 'layer_4.1.global_rep.0.pre_norm_ffn.4.weight', 'layer_4.1.global_rep.0.pre_norm_ffn.4.bias', 'layer_4.1.global_rep.1.pre_norm_mha.0.weight', 'layer_4.1.global_rep.1.pre_norm_mha.0.bias', 'layer_4.1.global_rep.1.pre_norm_mha.1.qkv_proj.weight', 'layer_4.1.global_rep.1.pre_norm_mha.1.qkv_proj.bias', 'layer_4.1.global_rep.1.pre_norm_mha.1.out_proj.weight', 'layer_4.1.global_rep.1.pre_norm_mha.1.out_proj.bias', 'layer_4.1.global_rep.1.pre_norm_ffn.0.weight', 'layer_4.1.global_rep.1.pre_norm_ffn.0.bias', 'layer_4.1.global_rep.1.pre_norm_ffn.1.weight', 'layer_4.1.global_rep.1.pre_norm_ffn.1.bias', 'layer_4.1.global_rep.1.pre_norm_ffn.4.weight', 'layer_4.1.global_rep.1.pre_norm_ffn.4.bias', 'layer_4.1.global_rep.2.pre_norm_mha.0.weight', 'layer_4.1.global_rep.2.pre_norm_mha.0.bias', 'layer_4.1.global_rep.2.pre_norm_mha.1.qkv_proj.weight', 'layer_4.1.global_rep.2.pre_norm_mha.1.qkv_proj.bias', 'layer_4.1.global_rep.2.pre_norm_mha.1.out_proj.weight', 'layer_4.1.global_rep.2.pre_norm_mha.1.out_proj.bias', 'layer_4.1.global_rep.2.pre_norm_ffn.0.weight', 'layer_4.1.global_rep.2.pre_norm_ffn.0.bias', 'layer_4.1.global_rep.2.pre_norm_ffn.1.weight', 'layer_4.1.global_rep.2.pre_norm_ffn.1.bias', 'layer_4.1.global_rep.2.pre_norm_ffn.4.weight', 'layer_4.1.global_rep.2.pre_norm_ffn.4.bias', 'layer_4.1.global_rep.3.pre_norm_mha.0.weight', 'layer_4.1.global_rep.3.pre_norm_mha.0.bias', 'layer_4.1.global_rep.3.pre_norm_mha.1.qkv_proj.weight', 'layer_4.1.global_rep.3.pre_norm_mha.1.qkv_proj.bias', 'layer_4.1.global_rep.3.pre_norm_mha.1.out_proj.weight', 'layer_4.1.global_rep.3.pre_norm_mha.1.out_proj.bias', 'layer_4.1.global_rep.3.pre_norm_ffn.0.weight', 'layer_4.1.global_rep.3.pre_norm_ffn.0.bias', 'layer_4.1.global_rep.3.pre_norm_ffn.1.weight', 'layer_4.1.global_rep.3.pre_norm_ffn.1.bias', 'layer_4.1.global_rep.3.pre_norm_ffn.4.weight', 'layer_4.1.global_rep.3.pre_norm_ffn.4.bias', 'layer_4.1.global_rep.4.weight', 'layer_4.1.global_rep.4.bias', 'layer_4.1.conv_proj.block.conv.weight', 'layer_4.1.conv_proj.block.norm.weight', 'layer_4.1.conv_proj.block.norm.bias', 'layer_4.1.fusion.block.conv.weight', 'layer_4.1.fusion.block.norm.weight', 'layer_4.1.fusion.block.norm.bias', 'layer_5.0.block.exp_1x1.block.conv.weight', 'layer_5.0.block.exp_1x1.block.norm.weight', 'layer_5.0.block.exp_1x1.block.norm.bias', 'layer_5.0.block.conv_3x3.block.conv.weight', 'layer_5.0.block.conv_3x3.block.norm.weight', 'layer_5.0.block.conv_3x3.block.norm.bias', 'layer_5.0.block.red_1x1.block.conv.weight', 'layer_5.0.block.red_1x1.block.norm.weight', 'layer_5.0.block.red_1x1.block.norm.bias', 'layer_5.1.local_rep.conv_3x3.block.conv.weight', 'layer_5.1.local_rep.conv_3x3.block.norm.weight', 'layer_5.1.local_rep.conv_3x3.block.norm.bias', 'layer_5.1.local_rep.conv_1x1.block.conv.weight', 'layer_5.1.global_rep.0.pre_norm_mha.0.weight', 'layer_5.1.global_rep.0.pre_norm_mha.0.bias', 'layer_5.1.global_rep.0.pre_norm_mha.1.qkv_proj.weight', 'layer_5.1.global_rep.0.pre_norm_mha.1.qkv_proj.bias', 'layer_5.1.global_rep.0.pre_norm_mha.1.out_proj.weight', 'layer_5.1.global_rep.0.pre_norm_mha.1.out_proj.bias', 'layer_5.1.global_rep.0.pre_norm_ffn.0.weight', 'layer_5.1.global_rep.0.pre_norm_ffn.0.bias', 'layer_5.1.global_rep.0.pre_norm_ffn.1.weight', 'layer_5.1.global_rep.0.pre_norm_ffn.1.bias', 'layer_5.1.global_rep.0.pre_norm_ffn.4.weight', 'layer_5.1.global_rep.0.pre_norm_ffn.4.bias', 'layer_5.1.global_rep.1.pre_norm_mha.0.weight', 'layer_5.1.global_rep.1.pre_norm_mha.0.bias', 'layer_5.1.global_rep.1.pre_norm_mha.1.qkv_proj.weight', 'layer_5.1.global_rep.1.pre_norm_mha.1.qkv_proj.bias', 'layer_5.1.global_rep.1.pre_norm_mha.1.out_proj.weight', 'layer_5.1.global_rep.1.pre_norm_mha.1.out_proj.bias', 'layer_5.1.global_rep.1.pre_norm_ffn.0.weight', 'layer_5.1.global_rep.1.pre_norm_ffn.0.bias', 'layer_5.1.global_rep.1.pre_norm_ffn.1.weight', 'layer_5.1.global_rep.1.pre_norm_ffn.1.bias', 'layer_5.1.global_rep.1.pre_norm_ffn.4.weight', 'layer_5.1.global_rep.1.pre_norm_ffn.4.bias', 'layer_5.1.global_rep.2.pre_norm_mha.0.weight', 'layer_5.1.global_rep.2.pre_norm_mha.0.bias', 'layer_5.1.global_rep.2.pre_norm_mha.1.qkv_proj.weight', 'layer_5.1.global_rep.2.pre_norm_mha.1.qkv_proj.bias', 'layer_5.1.global_rep.2.pre_norm_mha.1.out_proj.weight', 'layer_5.1.global_rep.2.pre_norm_mha.1.out_proj.bias', 'layer_5.1.global_rep.2.pre_norm_ffn.0.weight', 'layer_5.1.global_rep.2.pre_norm_ffn.0.bias', 'layer_5.1.global_rep.2.pre_norm_ffn.1.weight', 'layer_5.1.global_rep.2.pre_norm_ffn.1.bias', 'layer_5.1.global_rep.2.pre_norm_ffn.4.weight', 'layer_5.1.global_rep.2.pre_norm_ffn.4.bias', 'layer_5.1.global_rep.3.weight', 'layer_5.1.global_rep.3.bias', 'layer_5.1.conv_proj.block.conv.weight', 'layer_5.1.conv_proj.block.norm.weight', 'layer_5.1.conv_proj.block.norm.bias', 'layer_5.1.fusion.block.conv.weight', 'layer_5.1.fusion.block.norm.weight', 'layer_5.1.fusion.block.norm.bias', 'conv_1x1_exp.block.conv.weight', 'conv_1x1_exp.block.norm.weight', 'conv_1x1_exp.block.norm.bias', 'classifier.fc.weight', 'classifier.fc.bias']\n",
      "2025-12-07 15:58:35 - \u001b[32m\u001b[1mINFO   \u001b[0m - Trainable parameters: ['conv_1.block.conv.weight', 'conv_1.block.norm.weight', 'conv_1.block.norm.bias', 'layer_1.0.block.exp_1x1.block.conv.weight', 'layer_1.0.block.exp_1x1.block.norm.weight', 'layer_1.0.block.exp_1x1.block.norm.bias', 'layer_1.0.block.conv_3x3.block.conv.weight', 'layer_1.0.block.conv_3x3.block.norm.weight', 'layer_1.0.block.conv_3x3.block.norm.bias', 'layer_1.0.block.red_1x1.block.conv.weight', 'layer_1.0.block.red_1x1.block.norm.weight', 'layer_1.0.block.red_1x1.block.norm.bias', 'layer_2.0.block.exp_1x1.block.conv.weight', 'layer_2.0.block.exp_1x1.block.norm.weight', 'layer_2.0.block.exp_1x1.block.norm.bias', 'layer_2.0.block.conv_3x3.block.conv.weight', 'layer_2.0.block.conv_3x3.block.norm.weight', 'layer_2.0.block.conv_3x3.block.norm.bias', 'layer_2.0.block.red_1x1.block.conv.weight', 'layer_2.0.block.red_1x1.block.norm.weight', 'layer_2.0.block.red_1x1.block.norm.bias', 'layer_2.1.block.exp_1x1.block.conv.weight', 'layer_2.1.block.exp_1x1.block.norm.weight', 'layer_2.1.block.exp_1x1.block.norm.bias', 'layer_2.1.block.conv_3x3.block.conv.weight', 'layer_2.1.block.conv_3x3.block.norm.weight', 'layer_2.1.block.conv_3x3.block.norm.bias', 'layer_2.1.block.red_1x1.block.conv.weight', 'layer_2.1.block.red_1x1.block.norm.weight', 'layer_2.1.block.red_1x1.block.norm.bias', 'layer_2.2.block.exp_1x1.block.conv.weight', 'layer_2.2.block.exp_1x1.block.norm.weight', 'layer_2.2.block.exp_1x1.block.norm.bias', 'layer_2.2.block.conv_3x3.block.conv.weight', 'layer_2.2.block.conv_3x3.block.norm.weight', 'layer_2.2.block.conv_3x3.block.norm.bias', 'layer_2.2.block.red_1x1.block.conv.weight', 'layer_2.2.block.red_1x1.block.norm.weight', 'layer_2.2.block.red_1x1.block.norm.bias', 'layer_3.0.block.exp_1x1.block.conv.weight', 'layer_3.0.block.exp_1x1.block.norm.weight', 'layer_3.0.block.exp_1x1.block.norm.bias', 'layer_3.0.block.conv_3x3.block.conv.weight', 'layer_3.0.block.conv_3x3.block.norm.weight', 'layer_3.0.block.conv_3x3.block.norm.bias', 'layer_3.0.block.red_1x1.block.conv.weight', 'layer_3.0.block.red_1x1.block.norm.weight', 'layer_3.0.block.red_1x1.block.norm.bias', 'layer_3.1.local_rep.conv_3x3.block.conv.weight', 'layer_3.1.local_rep.conv_3x3.block.norm.weight', 'layer_3.1.local_rep.conv_3x3.block.norm.bias', 'layer_3.1.local_rep.conv_1x1.block.conv.weight', 'layer_3.1.global_rep.0.pre_norm_mha.0.weight', 'layer_3.1.global_rep.0.pre_norm_mha.0.bias', 'layer_3.1.global_rep.0.pre_norm_mha.1.qkv_proj.weight', 'layer_3.1.global_rep.0.pre_norm_mha.1.qkv_proj.bias', 'layer_3.1.global_rep.0.pre_norm_mha.1.out_proj.weight', 'layer_3.1.global_rep.0.pre_norm_mha.1.out_proj.bias', 'layer_3.1.global_rep.0.pre_norm_ffn.0.weight', 'layer_3.1.global_rep.0.pre_norm_ffn.0.bias', 'layer_3.1.global_rep.0.pre_norm_ffn.1.weight', 'layer_3.1.global_rep.0.pre_norm_ffn.1.bias', 'layer_3.1.global_rep.0.pre_norm_ffn.4.weight', 'layer_3.1.global_rep.0.pre_norm_ffn.4.bias', 'layer_3.1.global_rep.1.pre_norm_mha.0.weight', 'layer_3.1.global_rep.1.pre_norm_mha.0.bias', 'layer_3.1.global_rep.1.pre_norm_mha.1.qkv_proj.weight', 'layer_3.1.global_rep.1.pre_norm_mha.1.qkv_proj.bias', 'layer_3.1.global_rep.1.pre_norm_mha.1.out_proj.weight', 'layer_3.1.global_rep.1.pre_norm_mha.1.out_proj.bias', 'layer_3.1.global_rep.1.pre_norm_ffn.0.weight', 'layer_3.1.global_rep.1.pre_norm_ffn.0.bias', 'layer_3.1.global_rep.1.pre_norm_ffn.1.weight', 'layer_3.1.global_rep.1.pre_norm_ffn.1.bias', 'layer_3.1.global_rep.1.pre_norm_ffn.4.weight', 'layer_3.1.global_rep.1.pre_norm_ffn.4.bias', 'layer_3.1.global_rep.2.weight', 'layer_3.1.global_rep.2.bias', 'layer_3.1.conv_proj.block.conv.weight', 'layer_3.1.conv_proj.block.norm.weight', 'layer_3.1.conv_proj.block.norm.bias', 'layer_3.1.fusion.block.conv.weight', 'layer_3.1.fusion.block.norm.weight', 'layer_3.1.fusion.block.norm.bias', 'layer_4.0.block.exp_1x1.block.conv.weight', 'layer_4.0.block.exp_1x1.block.norm.weight', 'layer_4.0.block.exp_1x1.block.norm.bias', 'layer_4.0.block.conv_3x3.block.conv.weight', 'layer_4.0.block.conv_3x3.block.norm.weight', 'layer_4.0.block.conv_3x3.block.norm.bias', 'layer_4.0.block.red_1x1.block.conv.weight', 'layer_4.0.block.red_1x1.block.norm.weight', 'layer_4.0.block.red_1x1.block.norm.bias', 'layer_4.1.local_rep.conv_3x3.block.conv.weight', 'layer_4.1.local_rep.conv_3x3.block.norm.weight', 'layer_4.1.local_rep.conv_3x3.block.norm.bias', 'layer_4.1.local_rep.conv_1x1.block.conv.weight', 'layer_4.1.global_rep.0.pre_norm_mha.0.weight', 'layer_4.1.global_rep.0.pre_norm_mha.0.bias', 'layer_4.1.global_rep.0.pre_norm_mha.1.qkv_proj.weight', 'layer_4.1.global_rep.0.pre_norm_mha.1.qkv_proj.bias', 'layer_4.1.global_rep.0.pre_norm_mha.1.out_proj.weight', 'layer_4.1.global_rep.0.pre_norm_mha.1.out_proj.bias', 'layer_4.1.global_rep.0.pre_norm_ffn.0.weight', 'layer_4.1.global_rep.0.pre_norm_ffn.0.bias', 'layer_4.1.global_rep.0.pre_norm_ffn.1.weight', 'layer_4.1.global_rep.0.pre_norm_ffn.1.bias', 'layer_4.1.global_rep.0.pre_norm_ffn.4.weight', 'layer_4.1.global_rep.0.pre_norm_ffn.4.bias', 'layer_4.1.global_rep.1.pre_norm_mha.0.weight', 'layer_4.1.global_rep.1.pre_norm_mha.0.bias', 'layer_4.1.global_rep.1.pre_norm_mha.1.qkv_proj.weight', 'layer_4.1.global_rep.1.pre_norm_mha.1.qkv_proj.bias', 'layer_4.1.global_rep.1.pre_norm_mha.1.out_proj.weight', 'layer_4.1.global_rep.1.pre_norm_mha.1.out_proj.bias', 'layer_4.1.global_rep.1.pre_norm_ffn.0.weight', 'layer_4.1.global_rep.1.pre_norm_ffn.0.bias', 'layer_4.1.global_rep.1.pre_norm_ffn.1.weight', 'layer_4.1.global_rep.1.pre_norm_ffn.1.bias', 'layer_4.1.global_rep.1.pre_norm_ffn.4.weight', 'layer_4.1.global_rep.1.pre_norm_ffn.4.bias', 'layer_4.1.global_rep.2.pre_norm_mha.0.weight', 'layer_4.1.global_rep.2.pre_norm_mha.0.bias', 'layer_4.1.global_rep.2.pre_norm_mha.1.qkv_proj.weight', 'layer_4.1.global_rep.2.pre_norm_mha.1.qkv_proj.bias', 'layer_4.1.global_rep.2.pre_norm_mha.1.out_proj.weight', 'layer_4.1.global_rep.2.pre_norm_mha.1.out_proj.bias', 'layer_4.1.global_rep.2.pre_norm_ffn.0.weight', 'layer_4.1.global_rep.2.pre_norm_ffn.0.bias', 'layer_4.1.global_rep.2.pre_norm_ffn.1.weight', 'layer_4.1.global_rep.2.pre_norm_ffn.1.bias', 'layer_4.1.global_rep.2.pre_norm_ffn.4.weight', 'layer_4.1.global_rep.2.pre_norm_ffn.4.bias', 'layer_4.1.global_rep.3.pre_norm_mha.0.weight', 'layer_4.1.global_rep.3.pre_norm_mha.0.bias', 'layer_4.1.global_rep.3.pre_norm_mha.1.qkv_proj.weight', 'layer_4.1.global_rep.3.pre_norm_mha.1.qkv_proj.bias', 'layer_4.1.global_rep.3.pre_norm_mha.1.out_proj.weight', 'layer_4.1.global_rep.3.pre_norm_mha.1.out_proj.bias', 'layer_4.1.global_rep.3.pre_norm_ffn.0.weight', 'layer_4.1.global_rep.3.pre_norm_ffn.0.bias', 'layer_4.1.global_rep.3.pre_norm_ffn.1.weight', 'layer_4.1.global_rep.3.pre_norm_ffn.1.bias', 'layer_4.1.global_rep.3.pre_norm_ffn.4.weight', 'layer_4.1.global_rep.3.pre_norm_ffn.4.bias', 'layer_4.1.global_rep.4.weight', 'layer_4.1.global_rep.4.bias', 'layer_4.1.conv_proj.block.conv.weight', 'layer_4.1.conv_proj.block.norm.weight', 'layer_4.1.conv_proj.block.norm.bias', 'layer_4.1.fusion.block.conv.weight', 'layer_4.1.fusion.block.norm.weight', 'layer_4.1.fusion.block.norm.bias', 'layer_5.0.block.exp_1x1.block.conv.weight', 'layer_5.0.block.exp_1x1.block.norm.weight', 'layer_5.0.block.exp_1x1.block.norm.bias', 'layer_5.0.block.conv_3x3.block.conv.weight', 'layer_5.0.block.conv_3x3.block.norm.weight', 'layer_5.0.block.conv_3x3.block.norm.bias', 'layer_5.0.block.red_1x1.block.conv.weight', 'layer_5.0.block.red_1x1.block.norm.weight', 'layer_5.0.block.red_1x1.block.norm.bias', 'layer_5.1.local_rep.conv_3x3.block.conv.weight', 'layer_5.1.local_rep.conv_3x3.block.norm.weight', 'layer_5.1.local_rep.conv_3x3.block.norm.bias', 'layer_5.1.local_rep.conv_1x1.block.conv.weight', 'layer_5.1.global_rep.0.pre_norm_mha.0.weight', 'layer_5.1.global_rep.0.pre_norm_mha.0.bias', 'layer_5.1.global_rep.0.pre_norm_mha.1.qkv_proj.weight', 'layer_5.1.global_rep.0.pre_norm_mha.1.qkv_proj.bias', 'layer_5.1.global_rep.0.pre_norm_mha.1.out_proj.weight', 'layer_5.1.global_rep.0.pre_norm_mha.1.out_proj.bias', 'layer_5.1.global_rep.0.pre_norm_ffn.0.weight', 'layer_5.1.global_rep.0.pre_norm_ffn.0.bias', 'layer_5.1.global_rep.0.pre_norm_ffn.1.weight', 'layer_5.1.global_rep.0.pre_norm_ffn.1.bias', 'layer_5.1.global_rep.0.pre_norm_ffn.4.weight', 'layer_5.1.global_rep.0.pre_norm_ffn.4.bias', 'layer_5.1.global_rep.1.pre_norm_mha.0.weight', 'layer_5.1.global_rep.1.pre_norm_mha.0.bias', 'layer_5.1.global_rep.1.pre_norm_mha.1.qkv_proj.weight', 'layer_5.1.global_rep.1.pre_norm_mha.1.qkv_proj.bias', 'layer_5.1.global_rep.1.pre_norm_mha.1.out_proj.weight', 'layer_5.1.global_rep.1.pre_norm_mha.1.out_proj.bias', 'layer_5.1.global_rep.1.pre_norm_ffn.0.weight', 'layer_5.1.global_rep.1.pre_norm_ffn.0.bias', 'layer_5.1.global_rep.1.pre_norm_ffn.1.weight', 'layer_5.1.global_rep.1.pre_norm_ffn.1.bias', 'layer_5.1.global_rep.1.pre_norm_ffn.4.weight', 'layer_5.1.global_rep.1.pre_norm_ffn.4.bias', 'layer_5.1.global_rep.2.pre_norm_mha.0.weight', 'layer_5.1.global_rep.2.pre_norm_mha.0.bias', 'layer_5.1.global_rep.2.pre_norm_mha.1.qkv_proj.weight', 'layer_5.1.global_rep.2.pre_norm_mha.1.qkv_proj.bias', 'layer_5.1.global_rep.2.pre_norm_mha.1.out_proj.weight', 'layer_5.1.global_rep.2.pre_norm_mha.1.out_proj.bias', 'layer_5.1.global_rep.2.pre_norm_ffn.0.weight', 'layer_5.1.global_rep.2.pre_norm_ffn.0.bias', 'layer_5.1.global_rep.2.pre_norm_ffn.1.weight', 'layer_5.1.global_rep.2.pre_norm_ffn.1.bias', 'layer_5.1.global_rep.2.pre_norm_ffn.4.weight', 'layer_5.1.global_rep.2.pre_norm_ffn.4.bias', 'layer_5.1.global_rep.3.weight', 'layer_5.1.global_rep.3.bias', 'layer_5.1.conv_proj.block.conv.weight', 'layer_5.1.conv_proj.block.norm.weight', 'layer_5.1.conv_proj.block.norm.bias', 'layer_5.1.fusion.block.conv.weight', 'layer_5.1.fusion.block.norm.weight', 'layer_5.1.fusion.block.norm.bias', 'conv_1x1_exp.block.conv.weight', 'conv_1x1_exp.block.norm.weight', 'conv_1x1_exp.block.norm.bias', 'classifier.fc.weight', 'classifier.fc.bias']\n",
      "Model: MobileViT-xx_small\n",
      "Total parameters: 954,234\n",
      "Trainable parameters: 954,234\n",
      "Model size: ~3.82 MB (FP32)\n",
      "Model: MobileViT-xx_small\n",
      "Total parameters: 954,234\n",
      "Trainable parameters: 954,234\n",
      "Model size: ~3.82 MB (FP32)\n",
      "Output shape: torch.Size([2, 10])\n",
      "✓ Model created successfully!\n",
      "Output shape: torch.Size([2, 10])\n",
      "✓ Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create MobileViT model using the repository's infrastructure\n",
    "print(\"\\nInitializing MobileViT model...\")\n",
    "opts = create_opts_for_mobilevit(num_classes=NUM_CLASSES, mode=MODEL_MODE)\n",
    "\n",
    "try:\n",
    "    model = get_model(opts)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model information\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model: MobileViT-{MODEL_MODE}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: ~{total_params * 4 / 1e6:.2f} MB (FP32)\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    dummy_input = torch.randn(2, 3, 32, 32).to(device)\n",
    "    with torch.no_grad():\n",
    "        dummy_output = model(dummy_input)\n",
    "    print(f\"Output shape: {dummy_output.shape}\")\n",
    "    print(\"✓ Model created successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efe77f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up training components...\n",
      "Optimizer: AdamW (lr=0.002, weight_decay=0.01)\n",
      "Scheduler: Cosine annealing with 5 epoch warmup\n",
      "Loss: CrossEntropyLoss with label smoothing 0.1\n"
     ]
    }
   ],
   "source": [
    "# Setup optimizer and learning rate scheduler\n",
    "# Following the paper's training recipe (AdamW with cosine schedule)\n",
    "print(\"\\nSetting up training components...\")\n",
    "\n",
    "# Loss function with label smoothing (as in the paper)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# AdamW optimizer (as in the paper)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Cosine annealing learning rate scheduler with warmup\n",
    "# Paper uses 20k warmup iterations, we'll use 5 epochs for CIFAR-10\n",
    "warmup_epochs = 5\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = len(train_loader) * warmup_epochs\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        # Linear warmup\n",
    "        return (step / warmup_steps)\n",
    "    else:\n",
    "        # Cosine annealing\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return 0.5 * (1 + torch.cos(torch.tensor(progress * 3.14159)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr)\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
    "print(f\"Scheduler: Cosine annealing with {warmup_epochs} epoch warmup\")\n",
    "print(f\"Loss: CrossEntropyLoss with label smoothing 0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c74d1f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if batch_idx % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%',\n",
    "                'lr': f'{current_lr:.6f}'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc='Evaluating', leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    return test_loss, test_acc\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Starting Training\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████████████████████████| 391/391 [00:28<00:00, 13.93it/s, loss=2.294, acc=16.92%, lr=0.000400]\n",
      "Epoch 1/100: 100%|██████████████████████████████| 391/391 [00:28<00:00, 13.93it/s, loss=2.294, acc=16.92%, lr=0.000400]\n",
      "Epoch 2/100: 100%|██████████████████████████████| 391/391 [00:24<00:00, 15.89it/s, loss=1.929, acc=32.09%, lr=0.000800]\n",
      "Epoch 2/100: 100%|██████████████████████████████| 391/391 [00:24<00:00, 15.89it/s, loss=1.929, acc=32.09%, lr=0.000800]\n",
      "Epoch 3/100: 100%|██████████████████████████████| 391/391 [00:24<00:00, 15.91it/s, loss=1.745, acc=41.41%, lr=0.001200]\n",
      "Epoch 3/100: 100%|██████████████████████████████| 391/391 [00:24<00:00, 15.91it/s, loss=1.745, acc=41.41%, lr=0.001200]\n",
      "Epoch 4/100: 100%|██████████████████████████████| 391/391 [00:27<00:00, 14.02it/s, loss=1.641, acc=46.91%, lr=0.001600]\n",
      "Epoch 4/100: 100%|██████████████████████████████| 391/391 [00:27<00:00, 14.02it/s, loss=1.641, acc=46.91%, lr=0.001600]\n",
      "Epoch 5/100: 100%|██████████████████████████████| 391/391 [00:27<00:00, 14.24it/s, loss=1.564, acc=50.89%, lr=0.002000]\n",
      "Epoch 5/100: 100%|██████████████████████████████| 391/391 [00:27<00:00, 14.24it/s, loss=1.564, acc=50.89%, lr=0.002000]\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100:\n",
      "  Train Loss: 1.5643 | Train Acc: 50.89%\n",
      "  Test Loss: 1.4564 | Test Acc: 56.37%\n",
      "  Learning Rate: 0.002000\n",
      "  ✓ Best model saved! (Acc: 56.37%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████████████████████████| 391/391 [00:44<00:00,  8.70it/s, loss=1.461, acc=56.32%, lr=0.001999]\n",
      "Epoch 6/100: 100%|██████████████████████████████| 391/391 [00:44<00:00,  8.70it/s, loss=1.461, acc=56.32%, lr=0.001999]\n",
      "Epoch 7/100: 100%|██████████████████████████████| 391/391 [00:45<00:00,  8.53it/s, loss=1.381, acc=59.85%, lr=0.001998]\n",
      "Epoch 7/100: 100%|██████████████████████████████| 391/391 [00:45<00:00,  8.53it/s, loss=1.381, acc=59.85%, lr=0.001998]\n",
      "Epoch 8/100: 100%|██████████████████████████████| 391/391 [00:42<00:00,  9.11it/s, loss=1.327, acc=62.42%, lr=0.001995]\n",
      "Epoch 8/100: 100%|██████████████████████████████| 391/391 [00:42<00:00,  9.11it/s, loss=1.327, acc=62.42%, lr=0.001995]\n",
      "Epoch 9/100: 100%|██████████████████████████████| 391/391 [00:44<00:00,  8.71it/s, loss=1.281, acc=64.33%, lr=0.001991]\n",
      "Epoch 9/100: 100%|██████████████████████████████| 391/391 [00:44<00:00,  8.71it/s, loss=1.281, acc=64.33%, lr=0.001991]\n",
      "Epoch 10/100: 100%|█████████████████████████████| 391/391 [00:43<00:00,  8.95it/s, loss=1.248, acc=66.13%, lr=0.001986]\n",
      "Epoch 10/100: 100%|█████████████████████████████| 391/391 [00:43<00:00,  8.95it/s, loss=1.248, acc=66.13%, lr=0.001986]\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100:\n",
      "  Train Loss: 1.2480 | Train Acc: 66.13%\n",
      "  Test Loss: 1.1896 | Test Acc: 68.85%\n",
      "  Learning Rate: 0.001986\n",
      "  ✓ Best model saved! (Acc: 68.85%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|█████████████████████████████| 391/391 [00:41<00:00,  9.48it/s, loss=1.210, acc=67.99%, lr=0.001980]\n",
      "Epoch 11/100: 100%|█████████████████████████████| 391/391 [00:41<00:00,  9.48it/s, loss=1.210, acc=67.99%, lr=0.001980]\n",
      "Epoch 12/100: 100%|█████████████████████████████| 391/391 [00:41<00:00,  9.52it/s, loss=1.177, acc=69.63%, lr=0.001973]\n",
      "Epoch 12/100: 100%|█████████████████████████████| 391/391 [00:41<00:00,  9.52it/s, loss=1.177, acc=69.63%, lr=0.001973]\n",
      "Epoch 13/100: 100%|█████████████████████████████| 391/391 [00:40<00:00,  9.70it/s, loss=1.146, acc=71.09%, lr=0.001965]\n",
      "Epoch 13/100: 100%|█████████████████████████████| 391/391 [00:40<00:00,  9.70it/s, loss=1.146, acc=71.09%, lr=0.001965]\n",
      "Epoch 14/100: 100%|█████████████████████████████| 391/391 [00:40<00:00,  9.56it/s, loss=1.124, acc=72.05%, lr=0.001956]\n",
      "Epoch 14/100: 100%|█████████████████████████████| 391/391 [00:40<00:00,  9.56it/s, loss=1.124, acc=72.05%, lr=0.001956]\n",
      "Epoch 15/100: 100%|█████████████████████████████| 391/391 [00:41<00:00,  9.49it/s, loss=1.103, acc=73.15%, lr=0.001946]\n",
      "Epoch 15/100: 100%|█████████████████████████████| 391/391 [00:41<00:00,  9.49it/s, loss=1.103, acc=73.15%, lr=0.001946]\n",
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/100:\n",
      "  Train Loss: 1.1030 | Train Acc: 73.15%\n",
      "  Test Loss: 1.1083 | Test Acc: 72.79%\n",
      "  Learning Rate: 0.001946\n",
      "  ✓ Best model saved! (Acc: 72.79%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|█████████████████████████████| 391/391 [00:40<00:00,  9.57it/s, loss=1.081, acc=74.06%, lr=0.001935]\n",
      "Epoch 16/100: 100%|█████████████████████████████| 391/391 [00:40<00:00,  9.57it/s, loss=1.081, acc=74.06%, lr=0.001935]\n",
      "Epoch 17/100:  56%|████████████████▏            | 218/391 [00:26<00:15, 11.45it/s, loss=1.061, acc=75.44%, lr=0.001928]"
     ]
    }
   ],
   "source": [
    "# Training loop with checkpointing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs('./checkpoints', exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_loss': [],\n",
    "    'test_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Evaluate every 5 epochs or at the last epoch\n",
    "        if (epoch + 1) % 5 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "            test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "            \n",
    "            # Save history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}:')\n",
    "            print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "            print(f'  Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "            print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_acc': best_acc,\n",
    "                    'history': history,\n",
    "                }, './checkpoints/mobilevit_cifar10_best.pth')\n",
    "                print(f'  ✓ Best model saved! (Acc: {best_acc:.2f}%)')\n",
    "        \n",
    "        # Save checkpoint every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'history': history,\n",
    "            }, f'./checkpoints/mobilevit_cifar10_epoch_{epoch+1}.pth')\n",
    "            print(f'  Checkpoint saved at epoch {epoch+1}')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training Completed!\")\n",
    "    print(f\"Best Test Accuracy: {best_acc:.2f}% at epoch {best_epoch}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining interrupted by user.\")\n",
    "    print(f\"Best accuracy so far: {best_acc:.2f}% at epoch {best_epoch}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\nError during training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68756300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if len(history['train_loss']) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['test_loss'], label='Test Loss', marker='s')\n",
    "    axes[0].set_xlabel('Evaluation Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Test Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "    axes[1].plot(history['test_acc'], label='Test Acc', marker='s')\n",
    "    axes[1].set_xlabel('Evaluation Step')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Test Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Plot learning rate\n",
    "    axes[2].plot(history['lr'], marker='o', color='orange')\n",
    "    axes[2].set_xlabel('Evaluation Step')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('Learning Rate Schedule')\n",
    "    axes[2].grid(True)\n",
    "    axes[2].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./checkpoints/training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"  Best Test Accuracy: {best_acc:.2f}%\")\n",
    "    print(f\"  Final Train Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "    print(f\"  Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"\\nModel: MobileViT-{MODEL_MODE}\")\n",
    "    print(f\"Parameters: {total_params:,}\")\n",
    "    print(f\"Training completed in {NUM_EPOCHS} epochs\")\n",
    "else:\n",
    "    print(\"No training history to plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
